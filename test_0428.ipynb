{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_0428.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "17BTdla7M1DKfgR8e7mpWMW5_vRKfAAyy",
      "authorship_tag": "ABX9TyMK/pOEJWDMg6+SAYcAmFdE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lukaid/my_notebook/blob/main/test_0428.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADH25E6l68wl",
        "outputId": "a947cae9-8a27-4714-d08b-0c4a0969b16a"
      },
      "source": [
        "!bash"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bash: cannot set terminal process group (59): Inappropriate ioctl for device\n",
            "bash: no job control in this shell\n",
            "_id 2 --n_epoch 2 --rnn \n",
            "usage: train.py [-h] --model_fn MODEL_FN --train_fn TRAIN_FN [--gpu_id GPU_ID]\n",
            "                [--verbose VERBOSE] [--min_vocab_freq MIN_VOCAB_FREQ]\n",
            "                [--max_vocab_size MAX_VOCAB_SIZE] [--batch_size BATCH_SIZE]\n",
            "                [--n_epochs N_EPOCHS] [--word_vec_size WORD_VEC_SIZE]\n",
            "                [--dropout DROPOUT] [--max_length MAX_LENGTH] [--rnn]\n",
            "                [--hidden_size HIDDEN_SIZE] [--n_layers N_LAYERS] [--cnn]\n",
            "                [--use_batch_norm]\n",
            "                [--window_sizes [WINDOW_SIZES [WINDOW_SIZES ...]]]\n",
            "                [--n_filters [N_FILTERS [N_FILTERS ...]]]\n",
            "train.py: error: unrecognized arguments: Notebooks/review.sorted.uniq.refined.tok.shuf.train.tsv\n",
            " --n_epoch 2 --rnn \n",
            "usage: train.py [-h] --model_fn MODEL_FN --train_fn TRAIN_FN [--gpu_id GPU_ID]\n",
            "                [--verbose VERBOSE] [--min_vocab_freq MIN_VOCAB_FREQ]\n",
            "                [--max_vocab_size MAX_VOCAB_SIZE] [--batch_size BATCH_SIZE]\n",
            "                [--n_epochs N_EPOCHS] [--word_vec_size WORD_VEC_SIZE]\n",
            "                [--dropout DROPOUT] [--max_length MAX_LENGTH] [--rnn]\n",
            "                [--hidden_size HIDDEN_SIZE] [--n_layers N_LAYERS] [--cnn]\n",
            "                [--use_batch_norm]\n",
            "                [--window_sizes [WINDOW_SIZES [WINDOW_SIZES ...]]]\n",
            "                [--n_filters [N_FILTERS [N_FILTERS ...]]]\n",
            "train.py: error: unrecognized arguments: Notebooksreview.pth\n",
            "0 --n_epoch 2 --rnn \n",
            "usage: train.py [-h] --model_fn MODEL_FN --train_fn TRAIN_FN [--gpu_id GPU_ID]\n",
            "                [--verbose VERBOSE] [--min_vocab_freq MIN_VOCAB_FREQ]\n",
            "                [--max_vocab_size MAX_VOCAB_SIZE] [--batch_size BATCH_SIZE]\n",
            "                [--n_epochs N_EPOCHS] [--word_vec_size WORD_VEC_SIZE]\n",
            "                [--dropout DROPOUT] [--max_length MAX_LENGTH] [--rnn]\n",
            "                [--hidden_size HIDDEN_SIZE] [--n_layers N_LAYERS] [--cnn]\n",
            "                [--use_batch_norm]\n",
            "                [--window_sizes [WINDOW_SIZES [WINDOW_SIZES ...]]]\n",
            "                [--n_filters [N_FILTERS [N_FILTERS ...]]]\n",
            "train.py: error: unrecognized arguments: Notebooks/review.pth\n",
            "rnn \n",
            "|train| = 118314 |valid| = 29579\n",
            "|vocab| = 13042 |classes| = 2\n",
            "RNNClassifier(\n",
            "  (emb): Embedding(13042, 256)\n",
            "  (rnn): LSTM(256, 512, num_layers=4, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (generator): Linear(in_features=1024, out_features=2, bias=True)\n",
            "  (activation): LogSoftmax(dim=-1)\n",
            ")\n",
            "Epoch 1 - |param|=1.83e+03 |g_param|=3.19e-01 loss=2.2584e-01 accuracy=0.9186\n",
            "Validation - loss=2.4979e-01 accuracy=0.9005 best_loss=inf\n",
            "Epoch 2 - |param|=1.83e+03 |g_param|=2.46e-01 loss=1.8767e-01 accuracy=0.9338\n",
            "Validation - loss=2.1348e-01 accuracy=0.9142 best_loss=2.4979e-01\n",
            "8 --n_filters 100 100 100 100 100 100\n",
            "usage: train.py [-h] --model_fn MODEL_FN --train_fn TRAIN_FN [--gpu_id GPU_ID]\n",
            "                [--verbose VERBOSE] [--min_vocab_freq MIN_VOCAB_FREQ]\n",
            "                [--max_vocab_size MAX_VOCAB_SIZE] [--batch_size BATCH_SIZE]\n",
            "                [--n_epochs N_EPOCHS] [--word_vec_size WORD_VEC_SIZE]\n",
            "                [--dropout DROPOUT] [--max_length MAX_LENGTH] [--rnn]\n",
            "                [--hidden_size HIDDEN_SIZE] [--n_layers N_LAYERS] [--cnn]\n",
            "                [--use_batch_norm]\n",
            "                [--window_sizes [WINDOW_SIZES [WINDOW_SIZES ...]]]\n",
            "                [--n_filters [N_FILTERS [N_FILTERS ...]]]\n",
            "train.py: error: unrecognized arguments: Notebooks/review_rnn_cnn3_8.pth\n",
            "0 100 100 100 100 100\n",
            "|train| = 118314 |valid| = 29579\n",
            "|vocab| = 12967 |classes| = 2\n",
            "RNNClassifier(\n",
            "  (emb): Embedding(12967, 256)\n",
            "  (rnn): LSTM(256, 512, num_layers=4, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (generator): Linear(in_features=1024, out_features=2, bias=True)\n",
            "  (activation): LogSoftmax(dim=-1)\n",
            ")\n",
            "Epoch 1 - |param|=1.83e+03 |g_param|=3.89e-01 loss=2.1371e-01 accuracy=0.9251\n",
            "Validation - loss=1.8007e-01 accuracy=0.9367 best_loss=inf\n",
            "Epoch 2 - |param|=1.83e+03 |g_param|=2.89e-01 loss=1.7347e-01 accuracy=0.9397\n",
            "Validation - loss=1.5313e-01 accuracy=0.9456 best_loss=1.8007e-01\n",
            "Epoch 3 - |param|=1.83e+03 |g_param|=2.49e-01 loss=1.5632e-01 accuracy=0.9471\n",
            "Validation - loss=1.4926e-01 accuracy=0.9490 best_loss=1.5313e-01\n",
            "Epoch 4 - |param|=1.84e+03 |g_param|=2.38e-01 loss=1.3169e-01 accuracy=0.9556\n",
            "Validation - loss=1.4969e-01 accuracy=0.9515 best_loss=1.4926e-01\n",
            "Epoch 5 - |param|=1.84e+03 |g_param|=2.27e-01 loss=1.1653e-01 accuracy=0.9626\n",
            "Validation - loss=1.5646e-01 accuracy=0.9474 best_loss=1.4926e-01\n",
            "Epoch 6 - |param|=1.85e+03 |g_param|=2.61e-01 loss=1.0038e-01 accuracy=0.9663\n",
            "Validation - loss=1.7752e-01 accuracy=0.9459 best_loss=1.4926e-01\n",
            "Epoch 7 - |param|=1.85e+03 |g_param|=2.15e-01 loss=8.2293e-02 accuracy=0.9740\n",
            "Validation - loss=1.7135e-01 accuracy=0.9489 best_loss=1.4926e-01\n",
            "Epoch 8 - |param|=1.86e+03 |g_param|=2.31e-01 loss=7.3907e-02 accuracy=0.9744\n",
            "Validation - loss=2.0843e-01 accuracy=0.9466 best_loss=1.4926e-01\n",
            "Epoch 9 - |param|=1.86e+03 |g_param|=2.21e-01 loss=6.1310e-02 accuracy=0.9789\n",
            "Validation - loss=2.0845e-01 accuracy=0.9466 best_loss=1.4926e-01\n",
            "Epoch 10 - |param|=1.87e+03 |g_param|=1.87e-01 loss=5.2784e-02 accuracy=0.9815\n",
            "Validation - loss=2.3291e-01 accuracy=0.9479 best_loss=1.4926e-01\n",
            "CNNClassifier(\n",
            "  (emb): Embedding(12967, 256)\n",
            "  (feature_extractors): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(1, 100, kernel_size=(3, 256), stride=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(1, 100, kernel_size=(4, 256), stride=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Conv2d(1, 100, kernel_size=(5, 256), stride=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): Conv2d(1, 100, kernel_size=(6, 256), stride=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): Conv2d(1, 100, kernel_size=(7, 256), stride=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): Conv2d(1, 100, kernel_size=(8, 256), stride=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=600, out_features=2, bias=True)\n",
            "  (activation): LogSoftmax(dim=-1)\n",
            ")\n",
            "Epoch 1 - |param|=1.82e+03 |g_param|=3.88e+00 loss=2.4771e-01 accuracy=0.9105\n",
            "Validation - loss=1.8570e-01 accuracy=0.9298 best_loss=inf\n",
            "Epoch 2 - |param|=1.82e+03 |g_param|=4.88e+00 loss=2.3542e-01 accuracy=0.9171\n",
            "Validation - loss=3.9981e-01 accuracy=0.8467 best_loss=1.8570e-01\n",
            "Epoch 3 - |param|=1.83e+03 |g_param|=2.02e+00 loss=1.5907e-01 accuracy=0.9457\n",
            "Validation - loss=1.7038e-01 accuracy=0.9360 best_loss=1.8570e-01\n",
            "Epoch 4 - |param|=1.83e+03 |g_param|=2.61e+00 loss=1.5549e-01 accuracy=0.9486\n",
            "Validation - loss=3.2144e-01 accuracy=0.8885 best_loss=1.7038e-01\n",
            "Epoch 5 - |param|=1.83e+03 |g_param|=1.56e+00 loss=9.3687e-02 accuracy=0.9664\n",
            "Validation - loss=1.7687e-01 accuracy=0.9434 best_loss=1.7038e-01\n",
            "Epoch 6 - |param|=1.83e+03 |g_param|=2.77e+00 loss=2.3036e-01 accuracy=0.9499\n",
            "Validation - loss=3.3968e-01 accuracy=0.9225 best_loss=1.7038e-01\n",
            "Epoch 7 - |param|=1.83e+03 |g_param|=1.05e+00 loss=6.5983e-02 accuracy=0.9761\n",
            "Validation - loss=2.3113e-01 accuracy=0.9389 best_loss=1.7038e-01\n",
            "Epoch 8 - |param|=1.83e+03 |g_param|=9.41e-01 loss=5.5692e-02 accuracy=0.9814\n",
            "Validation - loss=2.4531e-01 accuracy=0.9387 best_loss=1.7038e-01\n",
            "Epoch 9 - |param|=1.83e+03 |g_param|=1.17e+00 loss=6.6790e-02 accuracy=0.9781\n",
            "Validation - loss=2.4925e-01 accuracy=0.9378 best_loss=1.7038e-01\n",
            "Epoch 10 - |param|=1.83e+03 |g_param|=1.52e+00 loss=6.8771e-02 accuracy=0.9770\n",
            "Validation - loss=2.7855e-01 accuracy=0.9408 best_loss=1.7038e-01\n",
            "00 100 100 100 100 100\n",
            "usage: train.py [-h] --model_fn MODEL_FN --train_fn TRAIN_FN [--gpu_id GPU_ID]\n",
            "                [--verbose VERBOSE] [--min_vocab_freq MIN_VOCAB_FREQ]\n",
            "                [--max_vocab_size MAX_VOCAB_SIZE] [--batch_size BATCH_SIZE]\n",
            "                [--n_epochs N_EPOCHS] [--word_vec_size WORD_VEC_SIZE]\n",
            "                [--dropout DROPOUT] [--max_length MAX_LENGTH] [--rnn]\n",
            "                [--hidden_size HIDDEN_SIZE] [--n_layers N_LAYERS] [--cnn]\n",
            "                [--use_batch_norm]\n",
            "                [--window_sizes [WINDOW_SIZES [WINDOW_SIZES ...]]]\n",
            "                [--n_filters [N_FILTERS [N_FILTERS ...]]]\n",
            "train.py: error: argument --window_sizes: invalid int value: '10--n_filters'\n",
            "100 100 100 100 100 100\n",
            "|train| = 118314 |valid| = 29579\n",
            "|vocab| = 13047 |classes| = 2\n",
            "RNNClassifier(\n",
            "  (emb): Embedding(13047, 256)\n",
            "  (rnn): LSTM(256, 512, num_layers=4, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (generator): Linear(in_features=1024, out_features=2, bias=True)\n",
            "  (activation): LogSoftmax(dim=-1)\n",
            ")\n",
            "Epoch 1 - |param|=1.83e+03 |g_param|=3.17e-01 loss=2.1677e-01 accuracy=0.9225\n",
            "Validation - loss=2.0538e-01 accuracy=0.9161 best_loss=inf\n",
            "Epoch 2 - |param|=1.84e+03 |g_param|=2.65e-01 loss=1.7702e-01 accuracy=0.9377\n",
            "Validation - loss=1.6368e-01 accuracy=0.9447 best_loss=2.0538e-01\n",
            "Epoch 3 - |param|=1.84e+03 |g_param|=2.27e-01 loss=1.5143e-01 accuracy=0.9482\n",
            "Validation - loss=1.5156e-01 accuracy=0.9480 best_loss=1.6368e-01\n",
            "Epoch 4 - |param|=1.84e+03 |g_param|=2.22e-01 loss=1.4081e-01 accuracy=0.9527\n",
            "Validation - loss=1.5655e-01 accuracy=0.9507 best_loss=1.5156e-01\n",
            "Epoch 5 - |param|=1.85e+03 |g_param|=2.10e-01 loss=1.1266e-01 accuracy=0.9611\n",
            "Validation - loss=1.7212e-01 accuracy=0.9478 best_loss=1.5156e-01\n",
            "Epoch 6 - |param|=1.85e+03 |g_param|=1.96e-01 loss=9.2342e-02 accuracy=0.9694\n",
            "Validation - loss=1.8464e-01 accuracy=0.9497 best_loss=1.5156e-01\n",
            "Epoch 7 - |param|=1.86e+03 |g_param|=2.50e-01 loss=8.7186e-02 accuracy=0.9718\n",
            "Validation - loss=1.6703e-01 accuracy=0.9480 best_loss=1.5156e-01\n",
            "Epoch 8 - |param|=1.86e+03 |g_param|=2.17e-01 loss=7.4885e-02 accuracy=0.9746\n",
            "Validation - loss=1.8865e-01 accuracy=0.9441 best_loss=1.5156e-01\n",
            "Epoch 9 - |param|=1.87e+03 |g_param|=2.24e-01 loss=6.2813e-02 accuracy=0.9778\n",
            "Validation - loss=2.1681e-01 accuracy=0.9486 best_loss=1.5156e-01\n",
            "Epoch 10 - |param|=1.87e+03 |g_param|=2.17e-01 loss=5.0963e-02 accuracy=0.9809\n",
            "Validation - loss=2.4730e-01 accuracy=0.9462 best_loss=1.5156e-01\n",
            "CNNClassifier(\n",
            "  (emb): Embedding(13047, 256)\n",
            "  (feature_extractors): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(1, 100, kernel_size=(5, 256), stride=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(1, 100, kernel_size=(6, 256), stride=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Conv2d(1, 100, kernel_size=(7, 256), stride=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): Conv2d(1, 100, kernel_size=(8, 256), stride=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): Conv2d(1, 100, kernel_size=(9, 256), stride=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): Conv2d(1, 100, kernel_size=(10, 256), stride=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=600, out_features=2, bias=True)\n",
            "  (activation): LogSoftmax(dim=-1)\n",
            ")\n",
            "Epoch 1 - |param|=1.83e+03 |g_param|=2.76e+00 loss=2.3192e-01 accuracy=0.9163\n",
            "Validation - loss=1.9194e-01 accuracy=0.9321 best_loss=inf\n",
            "Epoch [2/10]:  68%|████  | 626/925 [08:55<04:05,  1.22it/s, accuracy=0.937, loss=0.177, |g_param|=2.59, |param|=1.83e+3]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kfQWUlCYeqJ",
        "outputId": "f54b6770-c044-4114-fd19-a5ad7a2f4f1b"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.7/dist-packages (0.4.4)\n",
            "Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POBTK_k5qYeb"
      },
      "source": [
        "# rnn\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        word_vec_size,\n",
        "        hidden_size,\n",
        "        n_classes,\n",
        "        n_layers=4,\n",
        "        dropout_p=.3,\n",
        "    ):\n",
        "        self.input_size = input_size  # vocabulary_size, 단어의 개수, corpus에 따라 달라짐\n",
        "        self.word_vec_size = word_vec_size  # 워드 임베딩 벡터 (임베팅 후 벡터의 크기)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_classes = n_classes  # 아웃풋 클래스의 개수\n",
        "        self.n_layers = n_layers  # 레이어 수 보통 4개\n",
        "        self.dropout_p = dropout_p  # 드랍아웃 비율\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Linear Layer와 같은 개념\n",
        "        self.emb = nn.Embedding(input_size, word_vec_size)\n",
        "        # RNN\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=word_vec_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout_p,\n",
        "            batch_first=True,  # 배치 사이즈를 맨 처음에 넣어줌\n",
        "            bidirectional=True,  # 한번에 들어가는 거니까\n",
        "        )\n",
        "        self.generator = nn.Linear(\n",
        "            hidden_size * 2, n_classes)  # classification전에 차원축소\n",
        "        # We use LogSoftmax + NLLLoss instead of Softmax + CrossEntropy\n",
        "        self.activation = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # |x| = (batch_size, length, 1) # 1은 one-hot vector의 index\n",
        "        x = self.emb(x)\n",
        "        # |x| = (batch_size, length, word_vec_size)\n",
        "        # LSTM은 마지막 timestep의 hiddin, cell state(x) 와 output을 반환(_, many to one 이니까 무쓸모)\n",
        "        x, _ = self.rnn(x)\n",
        "        # |x| = (batch_size, length, hidden_size * 2)\n",
        "        # 슬라이싱, |x[:, -1]| = (batch_size, hidden_size * 2)\n",
        "        y = self.activation(self.generator(x[:, -1]))\n",
        "        # |y| = (batch_size, n_classes)\n",
        "\n",
        "        return y\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YLofhn4qVvJ"
      },
      "source": [
        "# cnn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        word_vec_size,\n",
        "        n_classes,\n",
        "        use_batch_norm=False,\n",
        "        dropout_p=.5,\n",
        "        window_sizes=[3, 4, 5],\n",
        "        n_filters=[100, 100, 100],\n",
        "    ):\n",
        "        # rnn은 batch_norm을 못쓰지만 cnn은 쓸수있음\n",
        "        # window_sizes는 몇단어 짜리 classifier인지\n",
        "        # n_filters는 몇개의 패턴인지\n",
        "\n",
        "        self.input_size = input_size  # vocabulary size\n",
        "        self.word_vec_size = word_vec_size\n",
        "        self.n_classes = n_classes\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.dropout_p = dropout_p\n",
        "        # window_size means that how many words a pattern covers.\n",
        "        self.window_sizes = window_sizes\n",
        "        # n_filters means that how many patterns to cover.\n",
        "        self.n_filters = n_filters\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(input_size, word_vec_size)\n",
        "        # Use nn.ModuleList to register each sub-modules.\n",
        "        self.feature_extractors = nn.ModuleList()  # 모듈을 담는 리스트....\n",
        "        # cnn 모듈을 만들어서 넣는 것\n",
        "        for window_size, n_filter in zip(window_sizes, n_filters):\n",
        "            self.feature_extractors.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=1,  # We only use one embedding layer.\n",
        "                        out_channels=n_filter,\n",
        "                        kernel_size=(window_size, word_vec_size),\n",
        "                    ),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm2d(\n",
        "                        n_filter) if use_batch_norm else nn.Dropout(dropout_p),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # An input of generator layer is max values from each filter.\n",
        "        self.generator = nn.Linear(sum(n_filters), n_classes)\n",
        "        # We use LogSoftmax + NLLLoss instead of Softmax + CrossEntropy\n",
        "        self.activation = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # |x| = (batch_size, length)\n",
        "        x = self.emb(x)\n",
        "        # |x| = (batch_size, length, word_vec_size)\n",
        "        min_length = max(self.window_sizes)\n",
        "        if min_length > x.size(1):\n",
        "            # Because some input does not long enough for maximum length of window size,\n",
        "            # we add zero tensor for padding.\n",
        "            pad = x.new(x.size(0), min_length - x.size(1),\n",
        "                        self.word_vec_size).zero_()\n",
        "            # |pad| = (batch_size, min_length - length, word_vec_size)\n",
        "            x = torch.cat([x, pad], dim=1)\n",
        "            # |x| = (batch_size, min_length, word_vec_size)\n",
        "\n",
        "        # In ordinary case of vision task, you may have 3 channels on tensor,\n",
        "        # but in this case, you would have just 1 channel,\n",
        "        # which is added by 'unsqueeze' method in below:\n",
        "        x = x.unsqueeze(1)\n",
        "        # |x| = (batch_size, 1, length, word_vec_size)\n",
        "\n",
        "        cnn_outs = []\n",
        "        for block in self.feature_extractors:\n",
        "            cnn_out = block(x)\n",
        "            # |cnn_out| = (batch_size, n_filter, length - window_size + 1, 1)\n",
        "\n",
        "            # In case of max pooling, we does not know the pooling size,\n",
        "            # because it depends on the length of the sentence.\n",
        "            # Therefore, we use instant function using 'nn.functional' package.\n",
        "            # This is the beauty of PyTorch. :)\n",
        "            cnn_out = nn.functional.max_pool1d(\n",
        "                input=cnn_out.squeeze(-1),\n",
        "                kernel_size=cnn_out.size(-2)\n",
        "            ).squeeze(-1)\n",
        "            # |cnn_out| = (batch_size, n_filter)\n",
        "            cnn_outs += [cnn_out]\n",
        "        # Merge output tensors from each convolution layer.\n",
        "        cnn_outs = torch.cat(cnn_outs, dim=-1)\n",
        "        # |cnn_outs| = (batch_size, sum(n_filters))\n",
        "        y = self.activation(self.generator(cnn_outs))\n",
        "        # |y| = (batch_size, n_classes)\n",
        "\n",
        "        return y\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm6vYCWdqPms"
      },
      "source": [
        "# utils\n",
        "\n",
        "def read_text(fn):\n",
        "    with open(fn, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "        labels, texts = [], []\n",
        "        for line in lines:\n",
        "            if line.strip() != '':\n",
        "                # The file should have tab delimited two columns.\n",
        "                # First column indicates label field,\n",
        "                # and second column indicates text field.\n",
        "                label, text = line.strip().split('\\t')\n",
        "                labels += [label]\n",
        "                texts += [text]\n",
        "\n",
        "    return labels, texts\n",
        "\n",
        "\n",
        "def get_grad_norm(parameters, norm_type=2):\n",
        "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
        "\n",
        "    total_norm = 0\n",
        "\n",
        "    try:\n",
        "        for p in parameters:\n",
        "            total_norm += (p.grad.data**norm_type).sum()\n",
        "        total_norm = total_norm ** (1. / norm_type)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    return total_norm\n",
        "\n",
        "\n",
        "def get_parameter_norm(parameters, norm_type=2):\n",
        "    total_norm = 0\n",
        "\n",
        "    try:\n",
        "        for p in parameters:\n",
        "            total_norm += (p.data**norm_type).sum()\n",
        "        total_norm = total_norm ** (1. / norm_type)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    return total_norm\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpYHMjq-qImh"
      },
      "source": [
        "# trainer\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from ignite.engine import Engine\n",
        "from ignite.engine import Events\n",
        "from ignite.metrics import RunningAverage\n",
        "from ignite.contrib.handlers.tqdm_logger import ProgressBar\n",
        "\n",
        "# from simple_ntc.utils import get_grad_norm, get_parameter_norm\n",
        "\n",
        "VERBOSE_SILENT = 0\n",
        "VERBOSE_EPOCH_WISE = 1\n",
        "VERBOSE_BATCH_WISE = 2\n",
        "\n",
        "\n",
        "class MyEngine(Engine):\n",
        "\n",
        "    def __init__(self, func, model, crit, optimizer, config):\n",
        "        # Ignite Engine does not have objects in below lines.\n",
        "        # Thus, we assign class variables to access these object, during the procedure.\n",
        "        self.model = model\n",
        "        self.crit = crit\n",
        "        self.optimizer = optimizer\n",
        "        self.config = config\n",
        "\n",
        "        super().__init__(func) # Ignite Engine only needs function to run.\n",
        "\n",
        "        self.best_loss = np.inf\n",
        "        self.best_model = None\n",
        "\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "    @staticmethod\n",
        "    def train(engine, mini_batch):\n",
        "        # You have to reset the gradients of all model parameters\n",
        "        # before to take another step in gradient descent.\n",
        "        engine.model.train() # Because we assign model as class variable, we can easily access to it.\n",
        "        engine.optimizer.zero_grad()\n",
        "\n",
        "        x, y = mini_batch.text, mini_batch.label\n",
        "        x, y = x.to(engine.device), y.to(engine.device)\n",
        "\n",
        "        x = x[:, :engine.config.max_length]\n",
        "\n",
        "        # Take feed-forward\n",
        "        y_hat = engine.model(x)\n",
        "\n",
        "        loss = engine.crit(y_hat, y)\n",
        "        loss.backward()\n",
        "\n",
        "        # Calculate accuracy only if 'y' is LongTensor,\n",
        "        # which means that 'y' is one-hot representation.\n",
        "        if isinstance(y, torch.LongTensor) or isinstance(y, torch.cuda.LongTensor):\n",
        "            accuracy = (torch.argmax(y_hat, dim=-1) == y).sum() / float(y.size(0))\n",
        "        else:\n",
        "            accuracy = 0\n",
        "\n",
        "        p_norm = float(get_parameter_norm(engine.model.parameters()))\n",
        "        g_norm = float(get_grad_norm(engine.model.parameters()))\n",
        "\n",
        "        # Take a step of gradient descent.\n",
        "        engine.optimizer.step()\n",
        "\n",
        "        return {\n",
        "            'loss': float(loss),\n",
        "            'accuracy': float(accuracy),\n",
        "            '|param|': p_norm,\n",
        "            '|g_param|': g_norm,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def validate(engine, mini_batch):\n",
        "        engine.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x, y = mini_batch.text, mini_batch.label\n",
        "            x, y = x.to(engine.device), y.to(engine.device)\n",
        "\n",
        "            x = x[:, :engine.config.max_length]\n",
        "\n",
        "            y_hat = engine.model(x)\n",
        "\n",
        "            loss = engine.crit(y_hat, y)\n",
        "\n",
        "            if isinstance(y, torch.LongTensor) or isinstance(y, torch.cuda.LongTensor):\n",
        "                accuracy = (torch.argmax(y_hat, dim=-1) == y).sum() / float(y.size(0))\n",
        "            else:\n",
        "                accuracy = 0\n",
        "\n",
        "        return {\n",
        "            'loss': float(loss),\n",
        "            'accuracy': float(accuracy),\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def attach(train_engine, validation_engine, verbose=VERBOSE_BATCH_WISE):\n",
        "        # Attaching would be repaeted for serveral metrics.\n",
        "        # Thus, we can reduce the repeated codes by using this function.\n",
        "        def attach_running_average(engine, metric_name):\n",
        "            RunningAverage(output_transform=lambda x: x[metric_name]).attach(\n",
        "                engine,\n",
        "                metric_name,\n",
        "            )\n",
        "\n",
        "        training_metric_names = ['loss', 'accuracy', '|param|', '|g_param|']\n",
        "\n",
        "        for metric_name in training_metric_names:\n",
        "            attach_running_average(train_engine, metric_name)\n",
        "\n",
        "        # If the verbosity is set, progress bar would be shown for mini-batch iterations.\n",
        "        # Without ignite, you can use tqdm to implement progress bar.\n",
        "        if verbose >= VERBOSE_BATCH_WISE:\n",
        "            pbar = ProgressBar(bar_format=None, ncols=120)\n",
        "            pbar.attach(train_engine, training_metric_names)\n",
        "\n",
        "        # If the verbosity is set, statistics would be shown after each epoch.\n",
        "        if verbose >= VERBOSE_EPOCH_WISE:\n",
        "            @train_engine.on(Events.EPOCH_COMPLETED)\n",
        "            def print_train_logs(engine):\n",
        "                print('Epoch {} - |param|={:.2e} |g_param|={:.2e} loss={:.4e} accuracy={:.4f}'.format(\n",
        "                    engine.state.epoch,\n",
        "                    engine.state.metrics['|param|'],\n",
        "                    engine.state.metrics['|g_param|'],\n",
        "                    engine.state.metrics['loss'],\n",
        "                    engine.state.metrics['accuracy'],\n",
        "                ))\n",
        "\n",
        "        validation_metric_names = ['loss', 'accuracy']\n",
        "        \n",
        "        for metric_name in validation_metric_names:\n",
        "            attach_running_average(validation_engine, metric_name)\n",
        "\n",
        "        # Do same things for validation engine.\n",
        "        if verbose >= VERBOSE_BATCH_WISE:\n",
        "            pbar = ProgressBar(bar_format=None, ncols=120)\n",
        "            pbar.attach(validation_engine, validation_metric_names)\n",
        "\n",
        "        if verbose >= VERBOSE_EPOCH_WISE:\n",
        "            @validation_engine.on(Events.EPOCH_COMPLETED)\n",
        "            def print_valid_logs(engine):\n",
        "                print('Validation - loss={:.4e} accuracy={:.4f} best_loss={:.4e}'.format(\n",
        "                    engine.state.metrics['loss'],\n",
        "                    engine.state.metrics['accuracy'],\n",
        "                    engine.best_loss,\n",
        "                ))\n",
        "\n",
        "    @staticmethod\n",
        "    def check_best(engine):\n",
        "        loss = float(engine.state.metrics['loss'])\n",
        "        if loss <= engine.best_loss: # If current epoch returns lower validation loss,\n",
        "            engine.best_loss = loss  # Update lowest validation loss.\n",
        "            engine.best_model = deepcopy(engine.model.state_dict()) # Update best model weights.\n",
        "\n",
        "    @staticmethod\n",
        "    def save_model(engine, train_engine, config, **kwargs):\n",
        "        torch.save(\n",
        "            {\n",
        "                'model': engine.best_model,\n",
        "                'config': config,\n",
        "                **kwargs\n",
        "            }, config.model_fn\n",
        "        )\n",
        "\n",
        "\n",
        "class Trainer():\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        model, crit, optimizer,\n",
        "        train_loader, valid_loader,\n",
        "    ):\n",
        "        train_engine = MyEngine(\n",
        "            MyEngine.train,\n",
        "            model, crit, optimizer, self.config\n",
        "        )\n",
        "        validation_engine = MyEngine(\n",
        "            MyEngine.validate,\n",
        "            model, crit, optimizer, self.config\n",
        "        )\n",
        "\n",
        "        MyEngine.attach(\n",
        "            train_engine,\n",
        "            validation_engine,\n",
        "            verbose=self.config.verbose\n",
        "        )\n",
        "\n",
        "        def run_validation(engine, validation_engine, valid_loader):\n",
        "            validation_engine.run(valid_loader, max_epochs=1)\n",
        "\n",
        "        train_engine.add_event_handler(\n",
        "            Events.EPOCH_COMPLETED, # event\n",
        "            run_validation, # function\n",
        "            validation_engine, valid_loader, # arguments\n",
        "        )\n",
        "        validation_engine.add_event_handler(\n",
        "            Events.EPOCH_COMPLETED, # event\n",
        "            MyEngine.check_best, # function\n",
        "        )\n",
        "\n",
        "        train_engine.run(\n",
        "            train_loader,\n",
        "            max_epochs=self.config.n_epochs,\n",
        "        )\n",
        "\n",
        "        model.load_state_dict(validation_engine.best_model)\n",
        "\n",
        "        return model\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4s0wFjhp7iz"
      },
      "source": [
        "# data_loader\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext.legacy import data\n",
        "\n",
        "\n",
        "class DataLoader(object):\n",
        "    '''\n",
        "    Data loader class to load text file using torchtext library.\n",
        "    '''\n",
        "\n",
        "    def __init__(\n",
        "        self, train_fn,\n",
        "        batch_size=64,\n",
        "        valid_ratio=.2,\n",
        "        device=-1,\n",
        "        max_vocab=999999,\n",
        "        min_freq=1,\n",
        "        use_eos=False,\n",
        "        shuffle=True,\n",
        "    ):\n",
        "        '''\n",
        "        DataLoader initialization.\n",
        "        :param train_fn: Train-set filename\n",
        "        :param batch_size: Batchify data fot certain batch size.\n",
        "        :param device: Device-id to load data (-1 for CPU)\n",
        "        :param max_vocab: Maximum vocabulary size\n",
        "        :param min_freq: Minimum frequency for loaded word.\n",
        "        :param use_eos: If it is True, put <EOS> after every end of sentence.\n",
        "        :param shuffle: If it is True, random shuffle the input data.\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        # Define field of the input file.\n",
        "        # The input file consists of two fields.\n",
        "        self.label = data.Field(\n",
        "            sequential=False,\n",
        "            use_vocab=True,\n",
        "            unk_token=None\n",
        "        )\n",
        "        self.text = data.Field(\n",
        "            use_vocab=True,\n",
        "            batch_first=True,\n",
        "            include_lengths=False,\n",
        "            eos_token='<EOS>' if use_eos else None,\n",
        "        )\n",
        "\n",
        "        # Those defined two columns will be delimited by TAB.\n",
        "        # Thus, we use TabularDataset to load two columns in the input file.\n",
        "        # We would have two separate input file: train_fn, valid_fn\n",
        "        # Files consist of two columns: label field and text field.\n",
        "        train, valid = data.TabularDataset(\n",
        "            path=train_fn,\n",
        "            format='tsv',\n",
        "            fields=[\n",
        "                ('label', self.label),\n",
        "                ('text', self.text),\n",
        "            ],\n",
        "        ).split(split_ratio=(1 - valid_ratio))\n",
        "\n",
        "        # Those loaded dataset would be feeded into each iterator:\n",
        "        # train iterator and valid iterator.\n",
        "        # We sort input sentences by length, to group similar lengths.\n",
        "        self.train_loader, self.valid_loader = data.BucketIterator.splits(\n",
        "            (train, valid),\n",
        "            batch_size=batch_size,\n",
        "            device='cuda:%d' % device if device >= 0 else 'cpu',\n",
        "            shuffle=shuffle,\n",
        "            sort_key=lambda x: len(x.text),\n",
        "            sort_within_batch=True,\n",
        "        )\n",
        "\n",
        "        # At last, we make a vocabulary for label and text field.\n",
        "        # It is making mapping table between words and indice.\n",
        "        self.label.build_vocab(train)\n",
        "        self.text.build_vocab(train, max_size=max_vocab, min_freq=min_freq)\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJP8LVAcOGO1"
      },
      "source": [
        "# train\n",
        "\n",
        "import argparse\n",
        "from argparse import Namespace\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from simple_ntc.trainer import Trainer\n",
        "from simple_ntc.data_loader import DataLoader\n",
        "\n",
        "from rnn import RNNClassifier\n",
        "from cnn import CNNClassifier\n",
        "\n",
        "\n",
        "\n",
        "def main(config):\n",
        "    loaders = DataLoader(\n",
        "        train_fn=config.train_fn,\n",
        "        batch_size=config.batch_size,\n",
        "        min_freq=config.min_vocab_freq,\n",
        "        max_vocab=config.max_vocab_size,\n",
        "        device=config.gpu_id\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        '|train| =', len(loaders.train_loader.dataset),\n",
        "        '|valid| =', len(loaders.valid_loader.dataset),\n",
        "    )\n",
        "    \n",
        "    vocab_size = len(loaders.text.vocab)\n",
        "    n_classes = len(loaders.label.vocab)\n",
        "    print('|vocab| =', vocab_size, '|classes| =', n_classes)\n",
        "\n",
        "    if config.rnn is False and config.cnn is False:\n",
        "        raise Exception('You need to specify an architecture to train. (--rnn or --cnn)')\n",
        "\n",
        "    if config.rnn:\n",
        "        # Declare model and loss.\n",
        "        model = RNNClassifier(\n",
        "            input_size=vocab_size,\n",
        "            word_vec_size=config.word_vec_size,\n",
        "            hidden_size=config.hidden_size,\n",
        "            n_classes=n_classes,\n",
        "            n_layers=config.n_layers,\n",
        "            dropout_p=config.dropout,\n",
        "        )\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "        crit = nn.NLLLoss()\n",
        "        print(model)\n",
        "\n",
        "        if config.gpu_id >= 0:\n",
        "            model.cuda(config.gpu_id)\n",
        "            crit.cuda(config.gpu_id)\n",
        "\n",
        "        rnn_trainer = Trainer(config)\n",
        "        rnn_model = rnn_trainer.train(\n",
        "            model,\n",
        "            crit,\n",
        "            optimizer,\n",
        "            loaders.train_loader,\n",
        "            loaders.valid_loader\n",
        "        )\n",
        "    if config.cnn:\n",
        "        # Declare model and loss.\n",
        "        model = CNNClassifier(\n",
        "            input_size=vocab_size,\n",
        "            word_vec_size=config.word_vec_size,\n",
        "            n_classes=n_classes,\n",
        "            use_batch_norm=config.use_batch_norm,\n",
        "            dropout_p=config.dropout,\n",
        "            window_sizes=config.window_sizes,\n",
        "            n_filters=config.n_filters,\n",
        "        )\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "        crit = nn.NLLLoss()\n",
        "        print(model)\n",
        "\n",
        "        if config.gpu_id >= 0:\n",
        "            model.cuda(config.gpu_id)\n",
        "            crit.cuda(config.gpu_id)\n",
        "\n",
        "        cnn_trainer = Trainer(config)\n",
        "        cnn_model = cnn_trainer.train(\n",
        "            model,\n",
        "            crit,\n",
        "            optimizer,\n",
        "            loaders.train_loader,\n",
        "            loaders.valid_loader\n",
        "        )\n",
        "\n",
        "    torch.save({\n",
        "        'rnn': rnn_model.state_dict() if config.rnn else None,\n",
        "        'cnn': cnn_model.state_dict() if config.cnn else None,\n",
        "        'config': config,\n",
        "        'vocab': loaders.text.vocab,\n",
        "        'classes': loaders.label.vocab,\n",
        "    }, config.model_fn)\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     config = {\n",
        "#         'model_fn': 'review.pth',\n",
        "#         'train_fn': 'review.sorted.uniq.refined.tok.shuf.train.tsv',\n",
        "#         'gpu_id': 0,\n",
        "#         'verbose': 2,\n",
        "#         'min_vocab_freq': 5,\n",
        "#         'max_vocab_size': 999999,\n",
        "#         'batch_size': 256,\n",
        "#         'n_epochs': 10,\n",
        "#         'word_vec_size': 256,\n",
        "#         'dropout': .3,\n",
        "#         'max_length': 256,\n",
        "#         'rnn': True,\n",
        "#         'hidden_size': 512,\n",
        "#         'n_layers': 4,\n",
        "#         'cnn': True,\n",
        "#         'use_batch_norm': True,\n",
        "#         'window_sizes': [3, 4, 5],\n",
        "#         'n_filters': [100, 100, 100]\n",
        "#     }\n",
        "#     config = Namespace(**config)\n",
        "#     main(config)\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "Lxplc33LOQVG",
        "outputId": "0e225b8d-d71b-4987-fa60-05060a116d24"
      },
      "source": [
        "config = {\n",
        "    'model_fn': 'review.pth',\n",
        "    'train_fn': 'review.sorted.uniq.refined.tok.shuf.train.tsv',\n",
        "    'gpu_id': 0,\n",
        "    'verbose': 2,\n",
        "    'min_vocab_freq': 5,\n",
        "    'max_vocab_size': 999999,\n",
        "    'batch_size': 256,\n",
        "    'n_epochs': 10,\n",
        "    'word_vec_size': 256,\n",
        "    'dropout': .3,\n",
        "    'max_length': 256,\n",
        "    'rnn': True,\n",
        "    'hidden_size': 512,\n",
        "    'n_layers': 4,\n",
        "    'cnn': True,\n",
        "    'use_batch_norm': True,\n",
        "    'window_sizes': [3, 4, 5],\n",
        "    'n_filters': [100, 100, 100]\n",
        "}\n",
        "\n",
        "config = Namespace(**config)\n",
        "\n",
        "print(config)\n",
        "\n",
        "main(config)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=256, cnn=True, dropout=0.3, gpu_id=0, hidden_size=512, max_length=256, max_vocab_size=999999, min_vocab_freq=5, model_fn='review.pth', n_epochs=10, n_filters=[100, 100, 100], n_layers=4, rnn=True, train_fn='review.sorted.uniq.refined.tok.shuf.train.tsv', use_batch_norm=True, verbose=2, window_sizes=[3, 4, 5], word_vec_size=256)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function tqdm.__del__ at 0x7fa916dc9b90>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tqdm/std.py\", line 1062, in __del__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tqdm/notebook.py\", line 233, in close\n",
            "    super(tqdm_notebook, self).close(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tqdm/std.py\", line 1237, in close\n",
            "    if self.disable:\n",
            "AttributeError: 'tqdm_notebook' object has no attribute 'disable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "|train| = 118314 |valid| = 29579\n",
            "|vocab| = 12970 |classes| = 2\n",
            "RNNClassifier(\n",
            "  (emb): Embedding(12970, 256)\n",
            "  (rnn): LSTM(256, 512, num_layers=4, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (generator): Linear(in_features=1024, out_features=2, bias=True)\n",
            "  (activation): LogSoftmax(dim=-1)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Current run is terminating due to exception: 'NoneType' object has no attribute 'replace'\n",
            "Engine run is terminating due to exception: 'NoneType' object has no attribute 'replace'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-9d890fed454c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-596a6ad33c3e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mloaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mloaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         )\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/simple_ntc/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, crit, optimizer, train_loader, valid_loader)\u001b[0m\n\u001b[1;32m    206\u001b[0m         train_engine.run(\n\u001b[1;32m    207\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Engine run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Current run is terminating due to exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m                 \u001b[0;31m# TODO: remove refs on batch to avoid high mem consumption ? -> need verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_fire_event\u001b[0;34m(self, event_name, *event_args, **event_kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mothers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mothers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/tqdm_logger.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, engine, logger, event_name)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mpbar_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_number_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar_total\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ignite/contrib/handlers/tqdm_logger.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self, pbar_total)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbar_total\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         self.pbar = self.pbar_cls(\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         )\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gui'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bar_format'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{l_bar}{bar}{r_bar}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bar_format'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bar_format'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{bar}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<bar/>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gui'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'replace'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj2P-nDv4m8d",
        "outputId": "12f75f63-8a42-427f-c9a3-be6483f2bc0f"
      },
      "source": [
        "config = {\n",
        "    'model_fn': 'review.pth',\n",
        "    'train_fn': 'review.sorted.uniq.refined.tok.shuf.train.tsv',\n",
        "    'gpu_id': 0,\n",
        "    'verbose': 2,\n",
        "    'min_vocab_freq': 5,\n",
        "    'max_vocab_size': 999999,\n",
        "    'batch_size': 256,\n",
        "    'n_epochs': 10,\n",
        "    'word_vec_size': 256,\n",
        "    'dropout': .3,\n",
        "    'max_length': 256,\n",
        "    'rnn': True,\n",
        "    'hidden_size': 512,\n",
        "    'n_layers': 4,\n",
        "    'cnn': True,\n",
        "    'use_batch_norm': True,\n",
        "    'window_sizes': [3, 4, 5],\n",
        "    'n_filters': [100, 100, 100]\n",
        "}\n",
        "\n",
        "config = Namespace(**config)\n",
        "\n",
        "print(config)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=256, cnn=True, dropout=0.3, gpu_id=0, hidden_size=512, max_length=256, max_vocab_size=999999, min_vocab_freq=5, model_fn='review.pth', n_epochs=10, n_filters=[100, 100, 100], n_layers=4, rnn=True, train_fn='review.sorted.uniq.refined.tok.shuf.train.tsv', use_batch_norm=True, verbose=2, window_sizes=[3, 4, 5], word_vec_size=256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTzUiLfqOSYm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}